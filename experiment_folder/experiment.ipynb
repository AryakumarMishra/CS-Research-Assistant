{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b003360f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --upgrade pymupdf langchain langchain-text-splitters pymupdf4llm langchain-huggingface sentence-transformers langchain-community faiss-cpu "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3adaa3a2",
   "metadata": {},
   "source": [
    "## EMBEDDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b47fe37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consider using the pymupdf_layout package for a greatly improved page layout analysis.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'# **Architectures, Generalization, and Future Frontiers: A Survey of** **Machine Learning for Deepfake Detection**\\n\\nAryakumar Ajay Mishra\\n\\n22108A0040\\nProgramme: B.Tech – Honours\\nProfessor Uma Jaishankar, Electronics and Computer Science\\n\\nVidyalankar Institute of Technology\\n\\nDate: July 25, 2025\\n\\n## **Abstract**\\n\\n\\nThis survey provides a comprehensive overview of the landscape of machine learning for deepfake\\ndetection. The rapid spread of hyper-realistic synthetic media created with advanced deep learning\\nmodels like Generative Adversarial Networks (GANs) poses a threat to personal security, political\\nstability, and social trust. This paper tracks the progression of detection methods from beginning\\nwith root methods based on spatial artifacts to advancing to more complex architectures that\\ninvestigate temporal inconsistencies. The central theme of this review is the vital challenge of\\ngeneralization—the inability of most detectors to work well on novel manipulation techniques. We\\nsurvey influential research articles that propose advanced paradigms like self-supervised learning,\\nrepresentation disentanglement, and active defenses to transcend this limitation. The study is\\ncarried out using a systematic review of 15 leading and recent peer-reviewed articles to extract the\\nstate of the art. Key results reveal that incredibly critical progress has been made, but the\\ntechnology arms race between detecting and creating deepfakes remains. The survey concludes by\\nhighlighting current limitations and avenues for future research, emphasizing the necessity for\\nmodels not just to be accurate but also robust, interpretable, and fair.\\n\\n## **1. Introduction**\\n\\n\\n**1.1 Motivation**\\n\\n\\nThe rapidity of progress and democratization of deep learning have enabled the production of\\nhyper-realistic synthetic media, commonly known as \"deepfakes.\" While technology is beneficial\\nin entertainment and e-commerce, manipulation thereof is a major threat [1]. Deepfakes can be\\nutilized to create fake narratives, impersonate individuals to defraud them, create non-consensual\\npornography, and spread misinformation to influence public opinion. [1] A chilling demonstration\\nwas seen in the 2022 Russia-Ukraine war, when a poor-quality deepfake of Ukrainian President\\nVolodymyr Zelenskyy appeared, urging soldiers to surrender.[3] Even though it was quickly\\ndismissed, the incident demonstrated the capacity of the technology to cause panic and induce\\n\\n\\nsuspicion. With the realism of the fakes increasing to the point of being indistinguishable from real\\nlife to the human eye, the threat of disruption to society grows exponentially, making the\\ndevelopment of accurate automated detection tools a priority.\\n\\n\\n**1.2 Scope of the Survey**\\n\\n\\nThis survey is dedicated to machine learning-based techniques for deepfake detection. It presents\\na formalized introduction to the area, with a common analytical focus on the generalization issue—\\nthe main failure of most detection models to generalize to manipulation tactics or data distributions\\noutside of training. The area covers a review of basic detection designs, the new techniques that\\nemerged to enhance generalization, the benchmark data sets, and the main challenges that define\\nthe area.\\n\\n\\n**1.3 Objectives**\\n\\n\\nThe principal aims of this questionnaire are to:\\n\\n\\n  - Offer a prelude to the fundamentals of deepfake creation and detection. Provide a\\n\\nsystematic summary of the most significant machine learning architectures used for\\ndetection, grouped by their fundamental approach.\\n\\n\\n  - Explain the fundamental problem of generalization of models and the sophisticated\\n\\ntechniques that were developed to address it.\\n\\n\\n  - Introduce the current trends, unresolved issues, and the limitations of the current work.\\n\\n\\n  - Determine areas of potential future research, including the need for improved reliability,\\n\\ninterpretability, and fairness.\\n\\n\\n**1.4 Methodology**\\n\\n\\nThe approach to this survey was a systematic literature reading of the notable work in deepfake\\ndetection. A set of 15 highly-referenced, recent peer-reviewed papers in top conferences and\\njournals were selected from academic repositories. The papers were read in order to synthesize the\\nevolutionary trajectory of detection architectures, determine the foremost technical challenges, and\\nmap the progression of solutions, with particular emphasis on the generalization problem. The\\nresults are presented thematically to give a brief overview of the state of the art.\\n\\n## **2. Background and Fundamentals**\\n\\n\\n**2.1 Basic Definitions and Concepts**\\n\\n\\nThe word \"deepfake\" is a portmanteau of \"deep learning\" and \"fake\"[1]. It is applied to describe\\nsynthetically created or manipulated media, usually images or videos, produced by using advanced\\ndeep learning algorithms. Sophisticated neural network architectures, specifically autoencoders\\nand Generative Adversarial Networks (GANs) [1], are the core technology utilized in most\\ndeepfake production. A GAN works by using a competitive process between two neural networks:\\na generator for creating synthetic media and a discriminator for trying to differentiate the fake\\n\\n\\ncontent from actual data [2]. The adversarial feedback loop compels the generator to generate\\nincreasingly realistic media.[2]\\n\\n\\nThe technology has evolved very fast from low-resolution, static image manipulations to highfidelity, dynamic video content that can fool both human viewers and machine-based systems [3].\\n\\n\\n**2.2 Technical Background**\\n\\n\\nThe pressing requirement for trusted countermeasures has sparked a technological \"arms race\"\\nbetween deepfake producers and classifiers.1 Deepfake detection is generally framed as a binary\\nclassification problem, in which a machine learning system is trained to examine some media and\\ndecide whether it is real (\"real\") or altered (\"fake\"). But this is a co-evolutionary process. No\\nsooner are detectors created to detect some form of forgery artifacts than producers change their\\nmanufacturing processes to remove those defects [1]. Detection thus continues to be a moving\\ntarget, with ongoing innovation needed to keep ahead of the ever-increasing sophistication of\\ngenerative models.\\n\\n\\n**3. Literature Review**\\n\\n\\n**3.1 Summary of Key Research and Methods**\\n\\n\\nDetection architecture has evolved reactively based on generation techniques. This expansion can\\nbe divided into three broad categories: spatial-domain analysis, temporal-domain analysis, and\\nproactive defenses.\\n3.1.1 Spatial-Domain Detection: Frame-Level Artifact Analysis. The first detectors were designed\\nto detect spatial artifacts in individual video frames.\\n\\n\\n  - Convolutional Neural Networks (CNNs): CNNs were the obvious place to begin. Basic\\n\\nmodels such as MesoNet, a small CNN, and Xception, a deep CNN model, were created to\\nidentify forgeries based on visual inconsistencies such as unnatural lighting, facial\\ndeformations, or artifacts at the blending edge where a fake face is spliced onto a video.[5]\\n\\n\\n  - Frequency-Domain Analysis: A more sophisticated approach makes the jump from the\\n\\npixel domain to the frequency domain. Generative models have a tendency to produce\\nspecific, periodic artifacts when up-sampling the image. Employing transforms like the\\nFast Fourier Transform (FFT), these otherwise latent spectral fingerprints can be made\\napparent and leveraged as a strong signal for detection.[6]\\n\\n\\n3.1.2 Temporal-Domain Detection: Utilize Inter-Frame Inconsistencies. As spatial artifacts\\nreduced, studies relied on the analysis of inconsistencies in video frames.\\n\\n\\n  - Recurrent Architectures (CNN+LSTM): CNN+LSTM represents an integration of a CNN\\n\\nfeature extractor and a Recurrent Neural Network (RNN) [2]. The RNN module analyzes\\ntemporal feature change, enabling it to detect abnormalities like unnatural flicker that can\\'t\\nbe detected in a single frame.[2]\\n\\n\\n  - Biological and Behavioral Cues: A significant category of temporal cues are biological. A\\n\\n\\nprominent example is the observation of an irregular rate of eye-blinking, since initial\\ndeepfakes were not able to replicate the normal behavior of this [8].\\n\\n\\n  - Vision Transformers (ViT): Such models as the Interpretable Spatial-Temporal Video\\n\\nTransformer (ISTVT) have been employed more recently.[9] Transformers leverage a selfattention mechanism to capture intricate, long-distance dependencies between different\\npatches within and beyond the frames, and thus are especially suited to identifying very\\ncomplex forgeries.[9]\\n\\n\\n3.1.3 Proactive Defenses: A Paradigm Shift\\nThe reactive tendency of the arms race has promoted a trend towards proactive defense. The most\\nglaring example is FaceGuard, a system based on digital watermarking using deep learning.[10]\\nThe basic idea is to insert a one-of-a-kind, invisible watermark into an actual image before\\npublication. This watermark is made robust against usual image processing but susceptible to\\ndeepfake attack. When the watermark is lost or destroyed, the image becomes tagged as a fake,\\nshifting the burden of proof from forgery artifact detection to authentication of a signal of\\nauthenticity [10].\\n\\n\\n**3.2 Comparative Analysis**\\n\\n\\nThe following table provides a comparative analysis of several key detection models and methods\\ndiscussed in this survey.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n|Model/Method|Key Reference|Core Methodology|Key Contribution|\\n|---|---|---|---|\\n|**Xception**|Rössler et al. (2019) [4]|CNN (Spatial Domain)|A <br>foundational<br>deep<br>CNN<br>baseline<br>for<br>detecting spatial artifacts.|\\n|**MesoNet**|Afchar et al. (2018)[5]|Shallow CNN (Spatial)|A <br>compact,<br>efficient<br>CNN<br>for<br>detecting<br>mesoscopic<br>forgery<br>features.|\\n|**CNN+LSTM**|Rani et al. (2023)2|Hybrid<br>CNN-RNN<br>(Temporal)|Combines spatial feature<br>extraction with temporal<br>sequence modeling.|\\n\\n\\n|ISTVT|Zhao et al. (2023) [9]|Video Transformer|Uses decomposed self-<br>attention to capture both<br>spatial and temporal<br>inconsistencies.|\\n|---|---|---|---|\\n|**PCL**|Zhao et al. (2021) [11]|Self-Supervised Learning|Learns<br>source<br>feature<br>inconsistency within an<br>image to detect forgery<br>stitching.|\\n|**CORE**|Ni et al. (2022) [12]|Self-Supervised Learning|Enforces<br>consistent<br>feature<br>representations<br>across<br>different<br>data<br>augmentations.|\\n|**FaceGuard**|Yu et al. (2021)[10]|Proactive Watermarking|Embeds<br>a <br>fragile<br>watermark<br>into<br>real<br>images<br>to<br>proactively<br>verify authenticity.|\\n\\n## **4. Current Trends and Challenges**\\n\\n**4.1 Emerging Trends**\\n\\n\\n\\n\\n\\nThe field is not controlled today by some top-level trends, with the aim of transcending the\\nrestrictions of the traditional methods. There is a clear shift away from simple CNN classifiers\\ntowards more powerful architectures like Vision Transformers that can learn long-range, intricate\\ndependencies.[9] The most conspicuous trend, however, is the focus on the development of models\\nthat can generalize. This has led to the widespread use of\\nself-supervised and contrastive learning models, which attempt to acquire the natural traits of real\\nand fake media without over-fitting on specific forgery attributes. [12] Finally, the concept of\\nproactive defense is increasingly gaining popularity as a way to break the cycle of reactive\\ndetection. [10]\\n\\n\\n**4.2 Open Issues and Limitations**\\n\\n\\nThe biggest problem with detecting deepfakes is generalizability. Detectors that have a nearperfect score when tested on in-dataset testing fail when cross-dataset tested on unseen forgery\\nmethods.[6] Detectors overfit on method-specific artifacts rather than learning the general concept\\nof what makes media fake.[6]\\n\\n\\nThis \"generalization gap\" is exacerbated by several factors:\\n\\n\\n  - Prior Benchmarks: Academic datasets do not reflect the deepfakes that exist \"in the wild.\"\\n\\nA new benchmark, Deepfake-Eval-2024, discovered that state-of-the-art model\\nperformance dropped as much as 50% on real-world data versus prior academic\\nbenchmarks [13].\\n\\n\\n  - Biases in Data and Models: Detectors can overfit on data that is unrelated to forgery, such\\n\\nas the position of the face in the frame (position bias) or the background scenery (content\\nbias) [14].\\n\\n\\n  - Robustness: Natural media is exposed to perturbations such as video compression and\\n\\nnoise that could potentially remove the weak artifacts that detectors are based on.\\nFurthermore, detectors can be attacked using adversarial attacks, where slight perturbations\\nare introduced into a deepfake so that it would be capable of deceiving the model.[1]\\n\\n## **5. Future Research Directions**\\n\\n\\n**5.1 Potential Improvements**\\n\\n\\nIn order to solve the issue of generalization, research is moving towards learning more invariant\\nand durable feature representations.\\n\\n\\n  - Self-Supervised and Contrastive Learning: Methods like Pair-wise Self-Consistency\\n\\nLearning (PCL) and COnsistent REpresentation Learning (CORE) make models learn\\ninnate consistencies. PCL presumes that every patch of a real image is from a consistent\\nsource signature, which is broken in a forgery.[11] CORE causes a model to learn\\nconsistent representations for different augmentations of one image so that it will ignore\\nsurface differences. [12]\\n\\n\\n  - Disentangled Representation Learning: This method attempts to disentangle the image\\n\\nfeatures directly into a forgery-agnostic content representation (e.g., expression, identity)\\nand a content-agnostic representation of forgery[15]. Learning a classifier on decoupled\\nforgery features makes the model more naturally content-variation robust[15]\\n\\n\\n  - Fine-Grained Analysis: Newer deepfakes contain extremely subtle, local artifacts.\\n\\nDetection is framed as a hierarchical classification problem by models like HiFi-Net, and\\ntherefore the model must learn a rich forgery feature tree from coarse to fine-grained to\\ndetect and localize better. [16]\\n\\n\\n**5.2 Unexplored Areas for Future Research**\\n\\n\\nBeyond improving generalization, several other areas require significant attention.\\n\\n\\n  - Interpretability (XAI): Detection systems must be interpretable so that they can be trusted\\n\\nin high-risk environments like journalism or law. \"Real\" or \"fake\" will no longer suffice;\\nmodels in the future will have to explain why a decision was made, for instance, by pointing\\nout the areas of an image that have been manipulated. [16]\\n\\n\\n  - Fairness and Bias: Detectors could be performance-varying across demographic groups\\n\\ndue to biases in training data, as shown. A non-fair detector is not a reliable detector. Future\\nwork will need to quantify and reduce such biases. Surprisingly, recent work suggests a\\ncausality where fairness improvement can enhance generalization, giving a \"one-stonehits-two-birds\" solution. [17]\\n\\n\\n  - Multimodality: Newer forgeries are more multimodal, more accurately coordinating audio\\n\\nand video manipulation. Future detection will need to combine evidence from several\\nmodalities and search for inconsistencies (e.g., lip-syncing and audio inconsistency) which\\nare not visible when examining a single modality. Beyond improving generalization,\\nseveral other areas require significant attention\\n\\n## **6. Conclusion**\\n\\n\\n**6.1 Key Takeaways**\\n\\n\\nThis survey has followed the progression of machine learning for the detection of deepfakes. From\\ninitial CNNs looking for spatial anomalies to recent Transformers and self-supervised methods\\nthat tackle the root problem of generalization, the technology path has been a linear and reactive\\none in order to keep pace with the ever-improving realism of deepfake creation, and the result is\\nan endless arms race. The lesson is that performance on a given benchmark is good, but\\ngeneralization to novel, unseen, and \"in-the-wild\" forgeries is the greatest single obstacle to their\\npractical application.\\n\\n\\n**6.2 Summary of Insights Gained**\\n\\n\\nThe history of detection techniques reflects a vital paradigm shift: the research goal is shifting\\nfrom only detecting well-known patterns of forgery to learning the intrinsic, invariant\\ncharacteristics of authenticity in itself. This is reflected in the direction towards sophisticated\\ntechniques such as representation disentanglement, self-supervised consistency training, and finegrained hierarchical analysis. As generative AI is increasing exponentially in our time, the menace\\nof synthetic media will keep growing accordingly.[3] Success in the detection of deepfakes in the\\ncoming times will not be the outcome of one miracle but a holistic approach. This involves ongoing\\ninnovation in learning generalizable representations, collaborative efforts to create more diverse\\nand less biased datasets, and a critical widening of attention from accuracy to reliability,\\ninterpretability, and fairness.\\n\\n## **References**\\n\\n\\n[1] Mirsky, Y., & Lee, W. (2021).\\n\\n\\n_The Creation and Detection of Deepfakes: A Survey._ ACM Computing Surveys, 54(1), 1-41.\\n[https://doi.org/10.1145/3425780](https://doi.org/10.1145/3425780)\\n\\n\\n[2] Rani, S., et al. (2023).\\n\\n\\n_Deepfake Video Detection System Using Deep Neural Networks._ 2023 IEEE International\\nConference on Innovations in Communication, and Computing (ICICC).\\n[https://www.researchgate.net/publication/370145676_Deepfake_Video_Detection_System_Using_Deep_](https://www.researchgate.net/publication/370145676_Deepfake_Video_Detection_System_Using_Deep_Neural_Networks)\\nNeural_Networks\\n\\n\\n[3] Chen, et. al. (2025)\\n\\n\\nMGFFD-VLM: Multi-Granularity Prompt Learning for Face Forgery Detection with VLM.\\n[https://arxiv.org/abs/2507.12232](https://arxiv.org/abs/2507.12232)\\n\\n\\n[4] Rössler, A., et al. (2019).\\n\\n\\n_FaceForensics++: Learning to Detect Manipulated Facial Images._ Proceedings of the IEEE/CVF\\nInternational Conference on Computer Vision\\n[(ICCV).(https://openaccess.thecvf.com/content_ICCV_2019/papers/Rossler_FaceForensics_Lear](https://openaccess.thecvf.com/content_ICCV_2019/papers/Rossler_FaceForensics_Learning_to_Detect_Manipulated_Facial_Images_ICCV_2019_paper.pdf)\\nning_to_Detect_Manipulated_Facial_Images_ICCV_2019_paper.pdf)\\n\\n\\n[5] Afchar, D., et al. (2018).\\n\\n\\n_MesoNet: a Compact Facial Video Forgery Detection Network._ IEEE International Workshop on\\n[Information Forensics and Security (WIFS). https://arxiv.org/abs/1809.00888](https://arxiv.org/abs/1809.00888)\\n\\n\\n[6] Luo, X., et al. (2021).\\n\\n\\n_Generalizing Face Forgery Detection With High-Frequency Features._ Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern Recognition\\n[(CVPR).(https://openaccess.thecvf.com/content/CVPR2021/papers/Luo_Generalizing_Face_For](https://openaccess.thecvf.com/content/CVPR2021/papers/Luo_Generalizing_Face_Forgery_Detection_With_High-Frequency_Features_CVPR_2021_paper.pdf)\\n[gery_Detection_With_High-Frequency_Features_CVPR_2021_paper.pdf)](https://openaccess.thecvf.com/content/CVPR2021/papers/Luo_Generalizing_Face_Forgery_Detection_With_High-Frequency_Features_CVPR_2021_paper.pdf)\\n\\n\\n[7] Tan, C., et al. (2024).\\n\\n\\n_Frequency-Aware Deepfake Detection: Improving Generalizability through Frequency Space_\\n_Learning._ Proceedings of the AAAI Conference on Artificial Intelligence.\\n[https://arxiv.org/abs/2403.07240](https://arxiv.org/abs/2403.07240)\\n\\n\\n[8] Li, Y., & Lyu, S. (2018).\\n\\n\\n_Exposing DeepFake Videos By Detecting Face Warping Artifacts._ arXiv:1811.00656.\\n[https://arxiv.org/abs/1811.00656](https://arxiv.org/abs/1811.00656)\\n\\n\\n[9] Zhao, C., et al. (2023).\\n\\n\\n_ISTVT: Interpretable Spatial-Temporal Video Transformer for Deepfake Detection._ IEEE\\nTransactions on Information Forensics and Security, 18, 1335\\n\\n1347.(https://ieeexplore.ieee.org/document/10024806)\\n\\n\\n[10] Yu, R., et al. (2021).\\n\\n\\n_FaceGuard:_ _Proactive_ _Deepfake_ _Detection._ 30th USENIX Security Symposium.\\n[https://arxiv.org/abs/2109.05673](https://arxiv.org/abs/2109.05673)\\n\\n\\n[11] Zhao, T., et al. (2021).\\n\\n\\n_Learning Self-Consistency for Deepfake Detection._ Proceedings of the IEEE/CVF International\\nConference on Computer Vision\\n[(ICCV).(https://openaccess.thecvf.com/content/ICCV2021/papers/Zhao_Learning_Self-](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhao_Learning_Self-Consistency_for_Deepfake_Detection_ICCV_2021_paper.pdf)\\n[Consistency_for_Deepfake_Detection_ICCV_2021_paper.pdf)](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhao_Learning_Self-Consistency_for_Deepfake_Detection_ICCV_2021_paper.pdf)\\n\\n\\n[12] Ni, Y., et al. (2022).\\n\\n\\n_CORE: COnsistent REpresentation Learning for Face Forgery Detection._ Proceedings of the\\nIEEE/CVF Conference on Computer.\\n[(https://openaccess.thecvf.com/content/CVPR2022W/WMF/papers/Ni_CORE_COnsistent_REpr](https://openaccess.thecvf.com/content/CVPR2022W/WMF/papers/Ni_CORE_COnsistent_REpresentation_Learning_for_Face_Forgery_Detection_CVPRW_2022_paper.pdf)\\n[esentation_Learning_for_Face_Forgery_Detection_CVPRW_2022_paper.pdf)](https://openaccess.thecvf.com/content/CVPR2022W/WMF/papers/Ni_CORE_COnsistent_REpresentation_Learning_for_Face_Forgery_Detection_CVPRW_2022_paper.pdf)\\n\\n\\n[13] Zhu, Z., et al. (2024).\\n\\n\\n_Deepfake-Eval-2024: A Comprehensive Benchmark for In-the-Wild Deepfake Detection._\\narXiv:2503.02857. https://arxiv.org/abs/2503.02857\\n\\n\\n[14] Guo, Y., et al. (2023).\\n\\n\\n_Hierarchical Fine-Grained Image Forgery Detection and Localization._ Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern Recognition\\n[(CVPR).(https://openaccess.thecvf.com/content/CVPR2023/papers/Guo_Hierarchical_Fine-](https://openaccess.thecvf.com/content/CVPR2023/papers/Guo_Hierarchical_Fine-Grained_Image_Forgery_Detection_and_Localization_CVPR_2023_paper.pdf)\\n[Grained_Image_Forgery_Detection_and_Localization_CVPR_2023_paper.pdf)](https://openaccess.thecvf.com/content/CVPR2023/papers/Guo_Hierarchical_Fine-Grained_Image_Forgery_Detection_and_Localization_CVPR_2023_paper.pdf)\\n\\n\\n[15] Yan, et al. (2024).\\n\\n\\n_Self-Supervised Feature Disentanglement for Deepfake Detection_\\n\\n\\nhttps://www.mdpi.com/2227-7390/13/12/2024\\n\\n\\n[16] Peipeng, et al, (2021)\\n\\n\\n_A Survey on Deepfake Video Detection_\\n\\n\\n_[https://www.researchgate.net/publication/350795842_A_Survey_on_Deepfake_Video_Detection](https://www.researchgate.net/publication/350795842_A_Survey_on_Deepfake_Video_Detection)_\\n\\n\\n[17] Cheng, H., et al. (2025).\\n\\n\\n_Fair Deepfake Detectors Can Generalize._ arXiv:2507.02645. https://arxiv.org/abs/2507.02645\\n\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pymupdf4llm\n",
    "md_text = pymupdf4llm.to_markdown('test/Honors_Research_Survey.pdf')\n",
    "md_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86ec08d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22196"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pathlib\n",
    "pathlib.Path(\"extracted_text.md\").write_bytes(md_text.encode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48c9dffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ARYAKUMAR-MISHRA\\Programming\\PROJECTS\\Machine-Learning\\AI_Research_Assistant\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import MarkdownTextSplitter\n",
    "\n",
    "splitter = MarkdownTextSplitter(chunk_size=100, chunk_overlap=20)\n",
    "md_text_split = splitter.create_documents([md_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31a16de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "vectorstore = FAISS.from_documents(\n",
    "    documents=md_text_split,\n",
    "    embedding=embeddings,\n",
    "    docstore=InMemoryDocstore()\n",
    ")\n",
    "\n",
    "vectorstore.save_local(\"faiss_index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9662cf4a",
   "metadata": {},
   "source": [
    "## RETRIEVAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6cdbe461",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "loaded_vectorstore = FAISS.load_local(\n",
    "    \"faiss_index\",\n",
    "    embeddings,\n",
    "    allow_dangerous_deserialization=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73731aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result 1:\n",
      "**6.2 Summary of Insights Gained**\n",
      "Result 2:\n",
      "## **2. Background and Fundamentals**\n",
      "\n",
      "\n",
      "**2.1 Basic Definitions and Concepts**\n",
      "Result 3:\n",
      "journals were selected from academic repositories. The papers were read in order to synthesize the\n"
     ]
    }
   ],
   "source": [
    "query = \"What is this paper about?\"\n",
    "\n",
    "docs = loaded_vectorstore.similarity_search(query, k=3)\n",
    "\n",
    "for i, doc in enumerate(docs):\n",
    "    print(f\"Result {i+1}:\")\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b81f768",
   "metadata": {},
   "source": [
    "#### Section-wise Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "76d38a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SECTION_QUERIES = {\n",
    "    \"problem_statement\": \"research problem, challenge addressed, research gap\",\n",
    "    \"motivation\": \"motivation, importance, why this problem matters\",\n",
    "    \"methodology\": \"proposed method, approach, system architecture\",\n",
    "    \"algorithms\": \"algorithm, model, pipeline, framework\",\n",
    "    \"limitations\": \"limitations, drawbacks, assumptions, failure cases\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8d0c1bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result 1:\n",
      "with root methods based on spatial artifacts to advancing to more complex architectures that\n",
      "Result 2:\n",
      "practical application.\n",
      "Result 3:\n",
      "Detection architecture has evolved reactively based on generation techniques. This expansion can\n",
      "Result 4:\n",
      "evolutionary trajectory of detection architectures, determine the foremost technical challenges,\n",
      "Result 5:\n",
      "|Model/Method|Key Reference|Core Methodology|Key Contribution|\n",
      "|---|---|---|---|\n"
     ]
    }
   ],
   "source": [
    "retriever = loaded_vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\n",
    "        \"k\": 5\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "docs = retriever.invoke(\n",
    "    SECTION_QUERIES[\"methodology\"]\n",
    ")\n",
    "\n",
    "for i, doc in enumerate(docs):\n",
    "    print(f\"Result {i+1}:\")\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd263c62",
   "metadata": {},
   "source": [
    "## TESTING LLM AND QA CHAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5b864b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROBLEM_STATEMENT_PROMPT = \"\"\"\n",
    "You are an AI research assistant analyzing a computer science research paper.\n",
    "\n",
    "TASK:\n",
    "Extract the PROBLEM STATEMENT of the paper.\n",
    "\n",
    "RULES (STRICT):\n",
    "- Use ONLY the provided context.\n",
    "- Do NOT use external knowledge.\n",
    "- Do NOT infer or assume anything.\n",
    "- If the problem is not explicitly stated, say:\n",
    "  \"The paper does not clearly state the problem.\"\n",
    "\n",
    "STYLE:\n",
    "- 3–5 concise sentences\n",
    "- Neutral academic tone\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "PROBLEM STATEMENT:\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e106fdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "MOTIVATION_PROMPT = \"\"\"\n",
    "You are an AI research assistant analyzing a computer science research paper.\n",
    "\n",
    "TASK:\n",
    "Explain the MOTIVATION of the paper (why the problem matters).\n",
    "\n",
    "RULES (STRICT):\n",
    "- Use ONLY the provided context.\n",
    "- Do NOT introduce background knowledge.\n",
    "- If motivation is not clearly discussed, say:\n",
    "  \"The paper does not clearly explain the motivation.\"\n",
    "\n",
    "STYLE:\n",
    "- 3–4 concise sentences\n",
    "- Academic tone\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "MOTIVATION:\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d5d992cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "METHODOLOGY_STATEMENT_PROMPT = \"\"\"\n",
    "You are an AI research assistant analyzing a computer science research paper.\n",
    "\n",
    "TASK:\n",
    "Extract the METHODOLOGY of the paper.\n",
    "\n",
    "RULES (STRICT):\n",
    "- Use ONLY the provided context.\n",
    "- Do NOT use external knowledge.\n",
    "- Do NOT infer or assume anything.\n",
    "- If the problem is not explicitly stated, say:\n",
    "  \"The paper does not clearly state the problem.\"\n",
    "\n",
    "STYLE:\n",
    "- 3–5 concise sentences\n",
    "- Neutral academic tone\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "METHODOLOGY:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "70be6226",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALGORITHMS_STATEMENT_PROMPT = \"\"\"\n",
    "You are an AI research assistant analyzing a computer science research paper.\n",
    "\n",
    "TASK:\n",
    "Extract the ALGORITHMS of the paper.\n",
    "\n",
    "RULES (STRICT):\n",
    "- Use ONLY the provided context.\n",
    "- Do NOT use external knowledge.\n",
    "- Do NOT infer or assume anything.\n",
    "- If the problem is not explicitly stated, say:\n",
    "  \"The paper does not clearly state the problem.\"\n",
    "\n",
    "STYLE:\n",
    "- 3–5 concise sentences\n",
    "- Neutral academic tone\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "ALGORITHMS:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9bf68d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "LIMITATIONS_STATEMENT_PROMPT = \"\"\"\n",
    "You are an AI research assistant analyzing a computer science research paper.\n",
    "\n",
    "TASK:\n",
    "Extract the LIMITATIONS of the paper.\n",
    "\n",
    "RULES (STRICT):\n",
    "- Use ONLY the provided context.\n",
    "- Do NOT use external knowledge.\n",
    "- Do NOT infer or assume anything.\n",
    "- If the problem is not explicitly stated, say:\n",
    "  \"The paper does not clearly state the problem.\"\n",
    "\n",
    "STYLE:\n",
    "- 3–5 concise sentences\n",
    "- Neutral academic tone\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "LIMITATIONS:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7baa9292",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_context(docs):\n",
    "    return \"\\n\\n\".join(\n",
    "        f\"[Chunk {i+1}]\\n{doc.page_content}\"\n",
    "        for i, doc in enumerate(docs)\n",
    "    )\n",
    "\n",
    "\n",
    "def run_llama(llm, prompt, max_tokens=200):\n",
    "    return llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "95f2ac3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "\n",
    "llm = Ollama(\n",
    "    model=\"llama3:8b\",\n",
    "    temperature=0.2,\n",
    "    num_predict=200\n",
    ")\n",
    "\n",
    "def generate_section(\n",
    "    section_name: str,\n",
    "    prompt_template: str,\n",
    "    retriever,\n",
    "    llm,\n",
    "    k: int = 5\n",
    "):\n",
    "    # 1. Retrieve section-specific chunks\n",
    "    docs = retriever.invoke(SECTION_QUERIES[section_name])\n",
    "\n",
    "    # 2. Build context\n",
    "    context = build_context(docs)\n",
    "\n",
    "    # 3. Fill prompt\n",
    "    prompt = prompt_template.format(context=context)\n",
    "\n",
    "    # 4. Generate output\n",
    "    response = run_llama(llm, prompt)\n",
    "\n",
    "    return response, docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a8aef36a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, the problem statement is:\n",
      "\n",
      "The paper does not clearly state the problem. However, it appears to be related to the \"generalization gap\" in a specific area of computer science, which is exacerbated by several factors and requires improved reliability and future research to address current limitations.\n"
     ]
    }
   ],
   "source": [
    "problem_output, problem_docs = generate_section(\n",
    "    section_name=\"problem_statement\",\n",
    "    prompt_template=PROBLEM_STATEMENT_PROMPT,\n",
    "    retriever=retriever,\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "print(problem_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7bed08d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, it appears that the motivation for this paper is to address biases in machine learning models. The authors mention that recent work suggests a need to quantify and reduce such biases, implying that current approaches are inadequate or incomplete.\n"
     ]
    }
   ],
   "source": [
    "motivation_output, motivation_docs = generate_section(\n",
    "    section_name=\"motivation\",\n",
    "    prompt_template=MOTIVATION_PROMPT,\n",
    "    retriever=retriever,\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "print(motivation_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91e2223",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
